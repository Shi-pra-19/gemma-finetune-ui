{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shi-pra-19/gemma-finetune-ui/blob/main/Gemma_2_Fine_Tuning_Dashboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install Required Libraries\n",
        "Install all the necessary Python packages for fine-tuning the Gemma-2B model using Streamlit, Hugging Face Transformers, and other related tools:"
      ],
      "metadata": {
        "id": "Rn5F0XX9KRAb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5MnYVuIylHe"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pyngrok transformers torch datasets trl peft bitsandbytes ngrok tqdm pandas --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVs52_iAz5-d"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade bitsandbytes transformers accelerate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Authentication and Integration Setup\n",
        "This cell handles authentication for key services used in fine-tuning and deployment:"
      ],
      "metadata": {
        "id": "AhWzMGW6KgVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XI50qpU0B6L"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(\"your-api-key\")\n",
        "\n",
        "from pyngrok import ngrok\n",
        "!ngrok authtoken your-auth-token\n",
        "\n",
        "import wandb\n",
        "wandb.login(key=\"your-api-key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma 2 Fine-Tuning Dashboard\n",
        "- Select a dataset and preview its content.\n",
        "\n",
        "- Configure training parameters such as batch size, learning rate, and training steps.\n",
        "\n",
        "- Choose data columns for input, context, and response.\n",
        "\n",
        "- Load the quantized Gemma-2B model with LoRA (Low-Rank Adaptation) support.\n",
        "\n",
        "- Visualize real-time training progress, including loss curves and status updates.\n",
        "\n",
        "- Train the model directly within the app using parameter-efficient fine-tuning."
      ],
      "metadata": {
        "id": "qiPXsrwnWGCO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh7XeIws0WhG"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, TrainerCallback\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "import plotly.graph_objects as go\n",
        "import re\n",
        "from random import randint\n",
        "from transformers import pipeline\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Gemma 2 Fine-Tuning Dashboard\")\n",
        "\n",
        "# Initialize session state\n",
        "if \"model_loaded\" not in st.session_state:\n",
        "    st.session_state.model_loaded = False\n",
        "\n",
        "if \"training_started\" not in st.session_state:\n",
        "    st.session_state.training_started = False\n",
        "\n",
        "# Sidebar: Dataset selection\n",
        "with st.sidebar:\n",
        "    st.header(\"Dataset Selection\")\n",
        "    datasets = {\n",
        "        \"Code Alpaca\": \"TokenBender/code_instructions_122k_alpaca_style\",\n",
        "        \"Databricks Dolly 15K\": \"databricks/databricks-dolly-15k\",\n",
        "        \"OpenAssistant OASST1\": \"OpenAssistant/oasst1\",\n",
        "        \"Guanaco OpenAssistant\": \"timdettmers/openassistant-guanaco\"\n",
        "    }\n",
        "    dataset_choice = st.selectbox(\"Select a dataset:\", list(datasets.keys()), index=None)\n",
        "\n",
        "# Load dataset and show a preview\n",
        "if dataset_choice:\n",
        "    dataset = load_dataset(datasets[dataset_choice], split=\"train\")\n",
        "    dataset = dataset.select(range(10))  # Limit to 10 for quick preview\n",
        "    df = dataset.to_pandas()\n",
        "    st.write(\"### Dataset Preview:\")\n",
        "    st.write(df.head(10))\n",
        "\n",
        "    # Let user select input, context (optional), and response fields\n",
        "    columns = df.columns.tolist()\n",
        "    input_field = st.selectbox(\"Select Input Field:\", columns)\n",
        "    context_field = st.selectbox(\"Select Context Field (Optional):\", [\"None\"] + columns)\n",
        "    response_field = st.selectbox(\"Select Response Field:\", columns)\n",
        "\n",
        "# Sidebar: Hyperparameter selection\n",
        "with st.sidebar:\n",
        "    st.header(\"Training Parameters\")\n",
        "    batch_size = st.slider(\"Batch Size\", 1, 16, 1)\n",
        "    learning_rate = st.number_input(\"Learning Rate\", min_value=1e-6, max_value=1e-2, value=2e-4, format=\"%.6f\")\n",
        "    gradient_accum_steps = st.slider(\"Gradient Accumulation Steps\", 1, 8, 4)\n",
        "    max_steps = st.slider(\"Max Steps\", 10, 1000, 100)\n",
        "\n",
        "# Load Model Button\n",
        "if dataset_choice and input_field and response_field:\n",
        "    if not st.session_state.model_loaded:\n",
        "        if st.button(\"Load Model\"):\n",
        "            model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16\n",
        "            )\n",
        "\n",
        "            st.session_state.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id, quantization_config=bnb_config, device_map={\"\": 0}\n",
        "            )\n",
        "            st.session_state.tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
        "            st.session_state.model_loaded = True\n",
        "            st.success(\"Model Loaded Successfully!\")\n",
        "else:\n",
        "    st.warning(\"Please select a dataset and configure input/response fields before loading the model.\")\n",
        "\n",
        "# Start Training Button\n",
        "if st.session_state.model_loaded and not st.session_state.training_started:\n",
        "    if dataset_choice and input_field and response_field:\n",
        "        if st.button(\"Start Training\"):\n",
        "            st.session_state.training_started = True\n",
        "    else:\n",
        "        st.warning(\"Make sure dataset and fields are selected before starting training.\")\n",
        "\n",
        "# Training Logic\n",
        "if st.session_state.training_started:\n",
        "    st.write(\"Preparing dataset for training...\")\n",
        "\n",
        "    def generate_prompt(data_point):\n",
        "        input_text = data_point[input_field]\n",
        "        response_text = data_point[response_field]\n",
        "        context_text = data_point[context_field] if context_field != \"None\" else None\n",
        "\n",
        "        if context_text:\n",
        "            return f\"<start_of_turn>user {input_text}\\nContext: {context_text}<end_of_turn>\\n<start_of_turn>model {response_text}<end_of_turn>\"\n",
        "        else:\n",
        "            return f\"<start_of_turn>user {input_text}<end_of_turn>\\n<start_of_turn>model {response_text}<end_of_turn>\"\n",
        "\n",
        "    text_column = [generate_prompt(data_point) for data_point in dataset]\n",
        "    dataset = dataset.add_column(\"prompt\", text_column).shuffle(seed=1234)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return st.session_state.tokenizer(examples[\"prompt\"], truncation=True, padding=True)\n",
        "\n",
        "    dataset = dataset.map(tokenize_function, batched=True)\n",
        "    dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "    train_data = dataset[\"train\"]\n",
        "    test_data = dataset[\"test\"]\n",
        "\n",
        "    st.session_state.model.gradient_checkpointing_enable()\n",
        "    st.session_state.model = prepare_model_for_kbit_training(st.session_state.model)\n",
        "\n",
        "    modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"down_proj\", \"gate_proj\", \"up_proj\"]\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=64,\n",
        "        lora_alpha=32,\n",
        "        target_modules=modules,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    st.session_state.model = get_peft_model(st.session_state.model, lora_config)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=gradient_accum_steps,\n",
        "        warmup_steps=0,\n",
        "        max_steps=max_steps,\n",
        "        learning_rate=learning_rate,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        save_strategy=\"epoch\",\n",
        "    )\n",
        "\n",
        "    st.write(\"Starting training...\")\n",
        "    progress_bar = st.progress(0)\n",
        "    log_area = st.empty()\n",
        "    loss_chart = st.empty()\n",
        "\n",
        "    class StreamlitCallback(TrainerCallback):\n",
        "        def __init__(self, progress_bar, log_area, max_steps, loss_chart):\n",
        "            self.progress_bar = progress_bar\n",
        "            self.log_area = log_area\n",
        "            self.max_steps = max_steps\n",
        "            self.loss_chart = loss_chart\n",
        "            self.losses = []\n",
        "\n",
        "        def on_step_end(self, args, state, control, **kwargs):\n",
        "            progress = state.global_step / self.max_steps\n",
        "            self.progress_bar.progress(progress)\n",
        "            self.log_area.write(f\"Step {state.global_step}/{self.max_steps}\")\n",
        "\n",
        "            if state.log_history and \"loss\" in state.log_history[-1]:\n",
        "                self.losses.append(state.log_history[-1][\"loss\"])\n",
        "                self.plot_loss()\n",
        "\n",
        "        def plot_loss(self):\n",
        "            fig = go.Figure()\n",
        "            fig.add_trace(go.Scatter(\n",
        "                y=self.losses,\n",
        "                mode='lines+markers',\n",
        "                name='Training Loss',\n",
        "                line=dict(color='royalblue')\n",
        "            ))\n",
        "            fig.update_layout(\n",
        "                title=\"Training Loss Curve\",\n",
        "                xaxis_title=\"Steps\",\n",
        "                yaxis_title=\"Loss\",\n",
        "                template=\"plotly_white\"\n",
        "            )\n",
        "            self.loss_chart.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=st.session_state.model,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=test_data,\n",
        "        peft_config=lora_config,\n",
        "        args=training_args,\n",
        "        data_collator=transformers.DataCollatorForLanguageModeling(\n",
        "            st.session_state.tokenizer, mlm=False\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    trainer.add_callback(StreamlitCallback(progress_bar, log_area, max_steps, loss_chart))\n",
        "    trainer.train()\n",
        "\n",
        "    st.success(\"Training Completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launching Streamlit with Ngrok"
      ],
      "metadata": {
        "id": "gNh3uoiXWbif"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFFHy-5d0XZ_"
      },
      "outputs": [],
      "source": [
        "!pkill -f ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95CpNuvw0dZB"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/dev/null &\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app is running at: {public_url}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}